{
 "metadata": {
  "name": "",
  "signature": "sha256:88475be10c71ac582779a6485660e709f48ea05675978ff70dd607c8626c89ff"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "\n",
      "\n",
      "<script language=\"javascript\">\n",
      "\n",
      "        function MouseRollover(MyImage,x) {\n",
      "        if (x=='0') {\n",
      "                document.getElementById(MyImage.id+\"t\").innerHTML=\"The effect of noise: No noise\";\n",
      "\n",
      "        }\n",
      "        else {\n",
      "                document.getElementById(MyImage.id+\"t\").innerHTML=\"The effect of noise: 30% noise\";\n",
      "}\n",
      "        MyImage.src = \"filters_corruption_\"+x+\".png\";\n",
      "        \n",
      "    }\n",
      "    \n",
      "        function MouseOut(MyImage,x) {\n",
      "        if (x=='0') {\n",
      "                document.getElementById(MyImage.id+\"t\").innerHTML=\"The effect of noise: No noise\";\n",
      "\n",
      "        }\n",
      "        else {\n",
      "                document.getElementById(MyImage.id+\"t\").innerHTML=\"The effect of noise: 30% noise\";\n",
      "}\n",
      "\n",
      "        MyImage.src = \"filters_corruption_\"+x+\".png\";\n",
      "    }\n",
      "</script>\n",
      "\n",
      "\n",
      "\n",
      "Autoencoders\n",
      "==============\n",
      "PhD Tutorial - 01/07/2014\n",
      "\n",
      "__James McMurray__\n",
      "\n",
      "(Most information taken from Stanford's UFLDL tutorial)\n",
      "\n",
      "\n",
      "<script>\n",
      "//Fix me later\n",
      "//<ol>\n",
      "//<li>Coffee</li>\n",
      "//<li>Milk</li>\n",
      "//</ol>\n",
      "var x = 0;\n",
      "\n",
      "function createPlan() {\n",
      "var outputs = document.getElementsByClassName(\"menu\");\n",
      "//var output = document.getElementById('menu'+x.toString());\n",
      "var menu = [\"What is an autoencoder?\",\"Neural networks recap\",\"Basic autoencoder\",\"Visualisation\", \"Stacked autoencoders\",\"Overcomplete autoencoders\",\"Denoising autoencoder\",\"Stacked denoising autoencoders\", \"Conclusion\"];\n",
      "\n",
      "for (x = 0; x < outputs.length; x++) { \n",
      " var htmltext=\"<h1>Plan</h1><ol>\";\n",
      "\n",
      "for (i = 0; i < menu.length; i++) { \n",
      "\n",
      "if (x>i) {\n",
      "\n",
      "    htmltext = htmltext + \"<li><span style=\\\"color:gray\\\">\" + menu[i] + \"</span></li>\" ;\n",
      "} else if (x==i) {\n",
      "    htmltext = htmltext + \"<li><span style=\\\"color:red\\\">\" + menu[i] + \"</span></li>\" ;\n",
      "} else {\n",
      "    htmltext = htmltext + \"<li><span style=\\\"color:black\\\">\" + menu[i] + \"</span></li>\" ;\n",
      "}\n",
      "\n",
      "} //end i loop\n",
      "\n",
      "    htmltext  = htmltext+\"</ol>\" ;\n",
      "    outputs[x].innerHTML = htmltext;\n",
      "\n",
      "}//end x loop\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "</script>\n",
      "\n",
      "\n",
      "<script> createPlan(); </script>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<p class=\"menu\"></p>    \n",
      "\n",
      "\n",
      "\n",
      "        "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What is an autoencoder?\n",
      "------------------------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* A neural network which attempts to learn a representation of the input data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* For example, for dimensionality reduction, contains hidden layers of lower dimensionality than the input"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* The output is a reconstruction of the input, from the hidden layers. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* So it is *unsupervised learning*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Used for dimensionality reduction, and for *unsupervised pre-training* in classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What is an autoencoder? : Simple example\n",
      "------------------------\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<img src=\"./ae.png\" style=\"width: 400px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Train on reconstruction of input (via back-propagation)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Values of hidden layer produced for input after training the weights, are the data after the dimensionality reduction."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<p class=\"menu\"></p>    \n",
      "\n",
      "\n",
      "\n",
      "        "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Neural networks recap\n",
      "------------------------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* A neural network consists of a series of layers: the input, a number of hidden layers, and the output"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Each *neuron* in the hidden layer, applies its weights to its input and passes them through an *activation function* to yield the output\n",
      "\n",
      "<img src=\"./SingleNeuron.png\" style=\"width: 400px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Output:     $~h(x) = f( W^T x) = f(\\sum_{i=1}^3 W_i x_i + b)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Common choices of the *activation function* are the sigmoid function and the tanh function."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Neural networks recap\n",
      "------------------------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* The outputs of one layer feed in to the next layer of the network."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<img src=\"./d2.png\" style=\"width: 800px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "###Training: Backpropagation\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Choose a loss function, for example, squared error loss: - for a single example\n",
      "\n",
      "<center>\n",
      "$J(W, b) = \\frac{1}{2} || h(x) - y || ^2 $\n",
      "</center>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Over $m$ examples, and adding weight decay for regularisation, this becomes:\n",
      "\n",
      "<center>\n",
      "$J(W, b) = \\frac{1}{m} \\sum_{i=1}^m \\left ( \\frac{1}{2} h(x_i) - y_i \\right )^2 + \\frac{\\lambda}{2} \\sum_{l=1}^{n_l -1} \\sum_{i=1}^{s_t} \\sum_{j=1}^{s_{t+1}} \\left ( W_{ji}^{(l)} \\right )^2 $\n",
      "\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Weight decay term may look complicated, but is just summing the squares of all the weights (except those from the biases)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Want to minimise $J(W,b)$ as a function of $W$, and $b$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Basic idea: \n",
      "\n",
      "    1. Do forward run \n",
      "    1. Calculate net error\n",
      "    1. Backpropagate error through the network, correctly assigning \"blame\" for the error, such that the best updates are made."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "###Training: Backpropagation: Algorithm\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Initialise weights randomly near zero\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Run feed-forward pass, calculating the output and the net loss from the output:\n",
      "<center>\n",
      "$ \\delta_i^{(n_l)} = \\frac{\\partial }{\\partial z_i^{(n_l)}} \\frac{1}{2} || y - h(x) || ^2 $\n",
      "</center>\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* For all previous layers: $~l=(n_l-1), ~ \\dots, ~ 2$ : calculate the loss for that neuron:\n",
      "\n",
      "<center>\n",
      "$\\delta_i^{(l)} = \\left ( \\sum_{j=1}^{s_{t+1}} W_{ji}^{(l)} \\delta_{j}^{l+1} \\right ) f' (z_i^{l} ) $\n",
      "\n",
      "</center>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Compute the partial derivatives:\n",
      "\n",
      "<center>\n",
      "$\\frac{ \\partial J}{\\partial W_{ij}^{(l)}} = a_{j}^{(l)} \\delta_i^{(l+1)}$\n",
      "\n",
      "</center>\n",
      " $~$\n",
      "\n",
      "<center>\n",
      "\n",
      "$\\frac{ \\partial J}{\\partial b_i^{(l)}} =  \\delta_i^{(l+1)}$\n",
      "\n",
      "</center>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* One can now apply gradient descent to optimise the loss function.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "###Training: Backpropagation: Main points\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Can train neural networks via gradient descent methods\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Backpropagation is an efficient way of calculating the partial derivatives\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Intuition: Propagates the error backwards, so bad weights are corrected\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* It isn't magic, just linear algebra!\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<p class=\"menu\"></p>    \n",
      "\n",
      "\n",
      "\n",
      "        "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Basic autoencoder\n",
      "-----------------\n",
      "\n",
      "<img src=\"./ae.png\" style=\"width: 500px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Basic autoencoder\n",
      "-----------------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Simplest autoencoder is deterministic."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Learns a deterministic mapping (encoding) of the input in to a hidden representation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* The latent representation can then be mapped back (through the decoder) to produce a reconstruction of the input."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Note this is not lossless, the aim is to minimise the reconstruction error across the training samples."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* A common constraint is _tied weights_. Whereby the decoder is constrained to be an inverse mapping of the encoder, such that $W' = W^T$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* If there is only one, __linear__ hidden layer, then it can be shown to reduce to PCA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<p class=\"menu\"></p>    \n",
      "\n",
      "\n",
      "\n",
      "        "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Visualisation\n",
      "-----------------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<img src=\"./aevis.png\" style=\"width: 250px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Visualise the input which would produce the maximal output of each hidden unit"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* In images, hope to see edge detectors, etc."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<p class=\"menu\"></p>    \n",
      "\n",
      "\n",
      "\n",
      "        "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Stacked autoencoders\n",
      "-----------------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Stacked autoencoders simply take the output of one autoencoding layer in to another."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Allows the greater expressive power of deep networks."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* This is very useful for classification tasks, as it may make the classes linearly seperable."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Stacks are trained greedily layer-wise."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Stacked autoencoders: Example\n",
      "------------------------------\n",
      "\n",
      "<img src=\"./comstack.png\" style=\"width: 400px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* In this example (taken from Stanford UFLDL tutorial) we have a stacked autoencoder with 2 hidden layers for a 3-class classification problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Each layer is trained greedily."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Stacked autoencoders: Example: First layer\n",
      "-------------------------------------------\n",
      "\n",
      "<img src=\"./stackl1.png\" style=\"width: 350px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* The first layer is trained with respect to reconstructing the input directly."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Stacked autoencoders: Example: Second layer\n",
      "-------------------------------------------\n",
      "\n",
      "<img src=\"./stackl2.png\" style=\"width: 350px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* The second layer is trained with respect to reconstructing the output of the first layer."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Stacked autoencoders: Example: Third layer\n",
      "-------------------------------------------\n",
      "\n",
      "<img src=\"./stackl3.png\" style=\"width: 400px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* The final layer is trained with respect to the classification error."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Stacked autoencoders: Example\n",
      "------------------------------\n",
      "\n",
      "<img src=\"./comstack.png\" style=\"width: 350px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* All the layers are then combined after their training (unsupervised pre-training) and the network is used for classification."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* One can also fine-tune the weights with respect to the classification error of the total network, now the weight should be in a better regime for gradient descent.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<p class=\"menu\"></p>    \n",
      "\n",
      "\n",
      "\n",
      "        "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Overcomplete autoencoders\n",
      "--------------------------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* One can also use hidden layers with a greater number of units than the input"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* This is an *overcomplete* autoencoder\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<img src=\"./overcomplete.png\" style=\"width: 400px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Overcomplete autoencoders\n",
      "--------------------------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* But with no constraints, could just learn identity function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* One common constraint is to enforce *sparsity* of the representation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Note this differs from just using a smaller number of hidden units, as the former could merge significant features, whereas the latter should select the most significant."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Another common approach is _Denoising Autoencoders_"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<p class=\"menu\"></p>    \n",
      "\n",
      "\n",
      "\n",
      "        "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Denoising autoencoders\n",
      "--------------------------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Add noise to training examples - attempt to reconstruct original example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Should make predictions robust to noise"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Avoids identity function problem mentioned above"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Denoising autoencoders\n",
      "--------------------------\n",
      "<img src=\"./daae.png\" style=\"width: 600px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h2 id=\"hovimt\">The effect of noise: No noise</h2>\n",
      "\n",
      "\n",
      "<img id='hovim' src=\"./filters_corruption_0.png\" style=\"width: 400px;\" onMouseOver=\"MouseRollover(this, '30')\" \n",
      "onMouseOut=\"MouseOut(this, '0')\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h2 id=\"hovim2t\">The effect of noise: 30% noise</h2>\n",
      "\n",
      "\n",
      "<img id='hovim2' src=\"./filters_corruption_30.png\" style=\"width: 400px;\" onMouseOver=\"MouseRollover(this, '0')\" \n",
      "onMouseOut=\"MouseOut(this, '30')\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<p class=\"menu\"></p>    \n",
      "\n",
      "\n",
      "\n",
      "        "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Stacked denoising autoencoders\n",
      "--------------------------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Can stack denoising autoencoders\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* It is common to apply small amount of noise to the output of *each* layer for further regularisation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Just as for stacked autoencoder without adding noise, the unsupervised pre-training is done layer per layer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* The whole network can then be finetuned with respect to the net output (or to a classification problem with an MLP output layer, etc.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<p class=\"menu\"></p>    \n",
      "\n",
      "\n",
      "\n",
      "        "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Conclusion\n",
      "-----------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Autoencoders provide a neural network based approach for dimensionality reduction\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Can also be used to improve classification by learning a (possibly overcomplete) representation that may be linearly separable\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Addition of noise and sparsity constraints prevent learning of identity function in overcomplete case\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Denoising autoencoders provide robustness against noise and missing values\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<center>\n",
      "<strong>Thank you for your time!</strong>\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "TODO\n",
      "======\n",
      "\n",
      "* Why can't use simple autoencoder\n",
      "* Add motivation section at start - dimensionality reduction\n",
      "* Use of RBM's for pretraining\n",
      "* Visualisation\n",
      "* Data whitening, etc.\n",
      "* Actual example - pre-training makes linearly seperable\n",
      "* Font size\n",
      "\n",
      "ipython nbconvert pres.ipynb --to slides --post serve\n",
      "\n",
      "<script> createPlan(); </script>\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "Plan\n",
      "=====\n",
      "\n",
      "1. <span style=\"color:red\">What is an autoencoder?</span> \n",
      "1. Neural networks recap\n",
      "1. Basic autoencoder\n",
      "1. Stacked autoencoders\n",
      "1. Overcomplete autoencoders\n",
      "1. Sparsity\n",
      "1. Denoising autoencoder\n",
      "1. Stacked denoising autoencoders"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}